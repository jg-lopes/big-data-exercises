{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "name": "BE4-Spark.ipynb",
    "colab": {
      "name": "ass2_spark_Joao_Gabriel_LOPES_DE_OLIVEIRA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fdLY2IbJu5_"
      },
      "source": [
        "<h1><center>Big Data Algorithms Techniques & Platforms</center></h1>\n",
        "\n",
        "<h2>\n",
        "<hr style=\" border:none; height:3px;\">\n",
        "<center>Assignment 2: Introduction to Spark</center>\n",
        "<hr style=\" border:none; height:3px;\">\n",
        "</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GD3rT2CvJu6A"
      },
      "source": [
        "# 1. Introduction\n",
        "\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "In this set of exercises you'll learn basic Spark programming skills that are necessary to develop simple, yet powerful, applications to be executed in a distributed environment.\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "The assignment is presented in this __Jupyter Notebook__, an interface that offers support for text, code, images and other media. Essentially, a Jupyter Notebook consists of multiple _cells_, either containing some text, like the one that you are reading, or code that you can execute. \n",
        "</font>\n",
        "</p>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4U4BCWZJu6B"
      },
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!tar zxvf spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark import SparkConf, SparkContext\n",
        "conf = SparkConf().setMaster(\"local\")\n",
        "sc = SparkContext(conf = conf)\n",
        "print(\"initialization successful!\")\n",
        "\n",
        "import numpy as np\n",
        "import random as rn\n",
        "\n",
        "seed_value=0\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BB7DUrvD5kCR"
      },
      "source": [
        "# B. Data import\n",
        "\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "Upload the folder data.zip inside the colab data folder and then execute the following code.\n",
        "</font>\n",
        "</p>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9Q0d7K35ICb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad3ef11d-3553-48b5-c210-2b049f52a197"
      },
      "source": [
        "!apt-get install unzip\n",
        "!unzip data.zip"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "unzip is already the newest version (6.0-21ubuntu1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 63 not upgraded.\n",
            "Archive:  data.zip\n",
            "replace data/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbHqimGOG8mD"
      },
      "source": [
        "# C. Support functions \n",
        "\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "Some support functions are provided. Read carefully the signatures of the fuctions.\n",
        "<ul>\n",
        "<li> $remove\\_non\\_letters(word)$\n",
        "<li> $load\\_stopwords(stopwords\\_file)$\n",
        "<li> $preprocess(text, stopwords)$\n",
        "<li> $word\\_count(words)$\n",
        "</ul>\n",
        "</font>\n",
        "</p>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08jVdk-T7JeF"
      },
      "source": [
        "import re\n",
        "# Regular expression for removing all non-letter characters in the file.\n",
        "regex = re.compile('[^a-zA-Z ]')\n",
        "\n",
        "\n",
        "'''\n",
        "Removes any non-letter character from the given word.\n",
        "\n",
        "INPUT:\n",
        "        word: A word\n",
        "\n",
        "OUTPUT:\n",
        "        the input word without the non-letter characters.\n",
        "\n",
        "'''\n",
        "def remove_non_letters(word):\n",
        "    return regex.sub('', word)\n",
        "\n",
        "\n",
        "'''\n",
        "INPUT: \n",
        "        stopwords_file: name of the file containing the stopwords.\n",
        "OUTPUT:\n",
        "        a Python list with the stopwords read from the file.\n",
        "'''\n",
        "def load_stopwords(stopwords_file):\n",
        "    stopwords = []\n",
        "    with open(stopwords_file) as file:\n",
        "        for sw in file:\n",
        "            stopwords.append(sw.strip())\n",
        "    return stopwords\n",
        "\n",
        "\n",
        "'''\n",
        "INPUT: \n",
        "        text: RDD where each element is a line of the input text file.\n",
        "        stopwords: Python list containing the stopwords.\n",
        "OUTPUT: \n",
        "        RDD where each element is a word from the input text file.\n",
        "'''\n",
        "def preprocess(text, stopwords) :\n",
        "  words = text.flatMap(lambda line: line.split(\" \")).map(lambda word: remove_non_letters(word)).filter(lambda word: len(word) > 0).map(lambda word: word.lower()).filter(lambda word: word not in stopwords)\n",
        "  return words\n",
        "\n",
        "'''\n",
        "Returns how many times a word appears in a RDD \n",
        "INPUT:\n",
        "        words: RDD, where each element is word from the input text file (preprocessing already done!).\n",
        "OUTPUT:\n",
        "        RDD, where each element is (w, occ), w is a word and occ the number of occurrences of w.\n",
        "        The RDD is sorted by value in decreasing order.\n",
        "'''\n",
        "\n",
        "def word_count(words):    \n",
        "    occs = words.map(lambda word: (word, 1))\\\n",
        "                .reduceByKey(lambda x, y: x+y)\\\n",
        "                .sortBy(lambda f: f[1], ascending=False)\n",
        "    return occs\n",
        "\n",
        "# Storing in stopwords the list of the stopwords that is provided\n",
        "stopwords = load_stopwords(\"./data/stopwords.txt\")\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdXYc3aMJu6x"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "<hr style=\"border:solid 2px;\">\n",
        "\n",
        "##  Exercise 1\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "In the folder _./data/bbc_ you'll find a collection of 50 documents from the BBC news website corresponding to stories in five topics. The five topics are:\n",
        "<ul>\n",
        "<li> _business_ \n",
        "<li>_entertainment_\n",
        "<li> _politics_\n",
        "<li> _sport_ \n",
        "<li> _tech_\n",
        "</ul>\n",
        "\n",
        "In the directory, the stories are text files (named: $\\_001.txt\\_$, $\\_002.txt\\_$, ...) organized into five directories, one for topic.\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "In this exercise, we want to create an **inverted index**. An inverted index is an essential component of a search engine. In fact, given any word, the inverted index allows the search engine to quickly retrieve all documents containing that word.\n",
        "\n",
        "An inverted index associates each word (you can find in the files) to the list of the names files the word occurs in.\n",
        "\n",
        "More precisely, for each word, the inverted index will have a list of the names  of the files (path relative to the folder _./data_) that contain the word. \n",
        "\n",
        "\n",
        "(family, \\[./data/bbc/tech/006.txt, ./data/bbc/entertainment/003.txt, ./data/bbc/entertainment/005.txt, ...\\]\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "The function $inverted\\_index$ has the following input and output:\n",
        "<ul>\n",
        "    <li> **Input.** A RDD $files$, where each element is $(f, content)$, $f$ being the name of a text file in the collection and $content$ being the content of that file; \n",
        "a Python list $stopwords$, containing the most common English stopwords.\n",
        "    <li> **Output.** A RDD, where each element is $(w, L)$, $w$ is a word and $L$ is the list of the names of the files containing $w$. The list must not contain duplicate file names.\n",
        "</ul>\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\" color='#91053d'>**Write the code of the function $inverted\\_index()$. The function must apply a sequence of RDD transformations to:**\n",
        "<ol>\n",
        "  <li> split the content of each file into its constituent words.\n",
        "  <li> lowercase each word.\n",
        "  <li> remove the non-letter characters from each word (you can use the function $remove\\_non\\_letters$ defined in Exercise 1).\n",
        "  <li> remove empty words.\n",
        "  <li> remove the stopwords.\n",
        "  <li> remove duplicate words.\n",
        "</ol>\n",
        "</font>\n",
        "</p>\n",
        "<hr style=\"border:solid 2px;\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4QgoDEAJu6y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5528dea9-0cc8-4e7b-969a-2da4b50ce5fa"
      },
      "source": [
        "'''\n",
        "INPUT:\n",
        "        files: RDD, each element is (f, content), where f is the name of a file in the collection and content is \n",
        "                its content.\n",
        "        stopwords: a Python list containing the stopwords.\n",
        "\n",
        "OUTPUT:y\n",
        "\n",
        "        a RDD, each element is (w, L), where w is a word and L is the list of the names of the files containing\n",
        "        w (without repetition).\n",
        "\n",
        "'''\n",
        "\n",
        "def inverted_index(files, stopwords):\n",
        "    '''############## WRITE YOUR CODE HERE ##############'''\n",
        "\n",
        "    #Split the content of each file into its constituent words and lowercase each word.\n",
        "    \n",
        "    # We first split the text into (f, w1), (f, w2) ... (f, wn) for all words in our file\n",
        "    # Splitting into so many pairs modularizes our following tasks (since they only deal with words)\n",
        "    # Likewise, splitting it into separate words allows for better paralellization in a distributed system\n",
        "    # Especially if working with limited memory and extremelly large files\n",
        "    line_split = files.flatMapValues(lambda text: text.split())\n",
        "    \n",
        "    # Applying lowercase transformation\n",
        "    lower_case = line_split.mapValues(lambda w: w.lower())\n",
        "    \n",
        "    # We could also implement it as a one-liner\n",
        "    #line_split = files.flatMapValues(lambda text: text.split()).mapValues(lambda w: w.lower())\n",
        "\n",
        "    #remove the non-letter characters from each word (you can use the function  𝑟𝑒𝑚𝑜𝑣𝑒_𝑛𝑜𝑛_𝑙𝑒𝑡𝑡𝑒𝑟𝑠  defined in Exercise 1).\n",
        "    no_non_letters = lower_case.mapValues(remove_non_letters)\n",
        "    \n",
        "    #remove empty words\n",
        "    no_empty = no_non_letters.filter(lambda pair: pair[1]) \n",
        "    \n",
        "    #remove the stopwords\n",
        "    no_stop_words = no_empty.filter(lambda pair: pair[1] not in stopwords)\n",
        "\n",
        "    #remove duplicate words\n",
        "    duplicates_removed = no_stop_words.distinct()\n",
        "\n",
        "    #produce the inverted index dictionary\n",
        "    output = duplicates_removed.map(lambda pair: (pair[1], pair[0])).groupByKey().mapValues(list)\n",
        "\n",
        "    return output\n",
        "\n",
        "    '''############## END OF THE EXERCISE ##############'''\n",
        "\n",
        "'''\n",
        "INPUT:\n",
        "        iindex: RDD containing the inverted index, as returned by the function inverted_index.\n",
        "        word: a word.\n",
        "\n",
        "OUTPUT:\n",
        "        prints the list of the files contain the given word.\n",
        "'''\n",
        "def lookup(iindex, word):\n",
        "    ld = iindex.sortByKey().lookup(word)\n",
        "    if len(ld) > 0:\n",
        "        print(\"The following documents contain the word '\",word,\"'\")\n",
        "        for d in sorted(ld[0]):\n",
        "            print(os.path.relpath(d[5:], os.getcwd()))\n",
        "    else:\n",
        "        print(\"No documents contain the word '\",word,\"'\")\n",
        "\n",
        "####################   GOOD TO KNOW  ####################\n",
        "# The Spark function wholeTextFiles loads into a RDD the content of the text files contained\n",
        "# in the given directory.\n",
        "# Each item of the RDD is a pair (f, content), where f is the name of a file and content is the content\n",
        "# of the file.\n",
        "#######################################################\n",
        "file_collection = sc.wholeTextFiles(\"./data/bbc/*\")        \n",
        "iindex = inverted_index(file_collection, stopwords)\n",
        "lookup(iindex, \"family\")\n",
        "\n",
        "################# EXPECTED OUTPUT #################\n",
        "#\n",
        "# data/bbc/entertainment/002.txt\n",
        "# data/bbc/entertainment/003.txt\n",
        "# data/bbc/entertainment/005.txt\n",
        "# data/bbc/politics/001.txt\n",
        "# data/bbc/sport/004.txt\n",
        "# data/bbc/tech/004.txt\n",
        "# data/bbc/tech/006.txt\n",
        "#\n",
        "###################################################\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The following documents contain the word ' family '\n",
            "data/bbc/entertainment/002.txt\n",
            "data/bbc/entertainment/003.txt\n",
            "data/bbc/entertainment/005.txt\n",
            "data/bbc/politics/001.txt\n",
            "data/bbc/sport/004.txt\n",
            "data/bbc/tech/004.txt\n",
            "data/bbc/tech/006.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "bjX3OKP-Ju6z"
      },
      "source": [
        "<hr style=\"border:solid 2px;\">\n",
        "\n",
        "##  Exercise 2\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "Given the BBC collection, we want to calculate the **co-occurrence matrix** $M$, such that $M[w_1][w_2]$ is the number of documents in which two words $w_1$ and $w_2$ appear in the same document (it does not matter if they are consecutive or not).\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "The function $co\\_occurrence\\_matrix()$ has the following input and output:\n",
        "<ul>\n",
        " <li> **Input.** A RDD $files$ and a Python list $stopwords$, as in the previous exercise.\n",
        " <li> **Output.** A RDD, where each element is $((w_1, w_2), occ)$, where $w_1$ and $w_2$ are words and $occ$ is the number of files in which the two words appear together.\n",
        "</ul>\n",
        "As in the case of the function $inverted\\_index()$, words must be lowercases, non-letter characters, empty words and stopwords should be removed.\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\" color='#91053d'>**Write the code of the function $co\\_occurrence\\_matrix()$. You can draw inspiration from the MapReduce algorithms that we discussed in class. Also, you can use the already implemented function $create\\_pairs()$ to generate all the possible pairs from a list of words. The function assumes that the words in the input list are sorted lexicographically.**\n",
        "<br>\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "<hr style=\"border:solid 2px;\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkIIh02tJu6z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fe5c3df-11a0-4c14-ea5f-6483f7a39afd"
      },
      "source": [
        "'''\n",
        "INPUT:\n",
        "        words: Python list containing words. IMPORTANT: the function assumes that the \n",
        "        list is sorted in lexicographic order.\n",
        "OUTPUT:\n",
        "        Python list containing all possible pairs from the given list.\n",
        "'''\n",
        "def create_pairs(words):\n",
        "    n = len(words)\n",
        "    output = []\n",
        "    for i in range(0, n):\n",
        "        for j in range(i+1, n):\n",
        "            output.append((words[i], words[j]))\n",
        "    return output\n",
        "\n",
        "'''\n",
        "INPUT:\n",
        "        files: RDD, each item is (f, content), where f is the name of a file and line is the content of the file.\n",
        "        stopwords: A RDD, each item is ((w1, w2), occ), where w1 and w2 are words and occ is the number of\n",
        "                    files in which w1 and w2 appear together.\n",
        "'''\n",
        "def co_occurrence_matrix(files, stopwords):\n",
        "    '''############## WRITE YOUR CODE HERE ##############'''\n",
        "    # For this exercise, since it does not specify specific variable and describes continuous transformations on the same RDD\n",
        "    # We will encode all our spark code into successive transformations of the same RDD to better display that everything is processed in-place\n",
        "    output = (files.flatMapValues(lambda text: text.split()) # Splitting into (L, w1), (L, w2)... \n",
        "                # As discussed before, possibly necessary for very large files and limited memory, also provides better paralelization\n",
        "               .mapValues(lambda w: w.lower()) # Lowercasing the words\n",
        "               .mapValues(remove_non_letters) # Removing non word characters\n",
        "               .filter(lambda pair: pair[1]) # Removing empty words -> pair[1] empty returns false -> filtered\n",
        "               .filter(lambda pair: pair[1] not in stopwords) # Removing stop words\n",
        "               .distinct() # Removing duplicates\n",
        "               \n",
        "               .groupByKey().mapValues(list) # Regrouping our filtered files per link as a list \n",
        "               # (L, [w1, w2, ..., wN])\n",
        "               \n",
        "               .mapValues(sorted) # Sorting the list of words alphabetically (lexicographic order)\n",
        "               \n",
        "               .flatMapValues(create_pairs) # Using the create_pairs function in order all (L, (w1, w2))\n",
        "               # for any w1 < w2 and w1 and w2 belonging to the same document L\n",
        "               # Once again, flatmapping might be use in very large files and for paralellization\n",
        "\n",
        "               .map(lambda pair_rdd: (pair_rdd[1], 1)) # Transforming (L, (w1, w2)) into ((w1,w2), 1)\n",
        "               # Will allow for transitioning into the classic word-counting example of MapReduce, but with a tuple instead of a single word\n",
        "\n",
        "               # Using an addition reduce to count the quantity of the pairs\n",
        "               .reduceByKey(lambda x, y: x + y))\n",
        "    \n",
        "    return output\n",
        "    '''############## END OF THE EXERCISE ##############'''\n",
        "\n",
        "\n",
        "file_collection = sc.wholeTextFiles(\"./data/bbc/*\")\n",
        "output = co_occurrence_matrix(file_collection, stopwords)    \n",
        "output.takeOrdered(10, key = lambda x: -x[1])\n",
        "\n",
        "################# EXAMPLE OF FORMAT FOR THE EXPECTED OUTPUT #################\n",
        "################# THIS IS NOT THE SOLUTION #################\n",
        "#\n",
        "#[(('a', 'b'), 3),\n",
        "# (('c', 'f'), 12),\n",
        "# ... ]\n",
        "#\n",
        "###################################################\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('also', 'said'), 24),\n",
              " (('said', 'world'), 20),\n",
              " (('new', 'said'), 19),\n",
              " (('said', 'year'), 17),\n",
              " (('also', 'world'), 17),\n",
              " (('one', 'said'), 16),\n",
              " (('last', 'said'), 15),\n",
              " (('said', 'set'), 15),\n",
              " (('said', 'time'), 15),\n",
              " (('back', 'said'), 14)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "6IOoMHo1Ju61"
      },
      "source": [
        "<hr style=\" border:solid 2px;\">\n",
        "\n",
        "##  Exercise 3 - OPTIONAL - enjoy with what you just wrote\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "We want to code a function $term\\_freq$ that computes the frequency of each word in a \n",
        "text document. \n",
        "More precisely, given a document $d$ and a word $w$ in that document, we want to \n",
        "compute its frequency $tf(w, d)$, as follows:\n",
        "    \n",
        "<p>    \n",
        "$$ tf(w, d) = \\frac{f_{w, d}}{\\sum\\limits_{w^\\prime \\in d} f_{w^\\prime, d}}$$\n",
        "</p>\n",
        "\n",
        "where $f_{w, d}$ is the number of occurrences of word $w$ in $d$.\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "<p>\n",
        "<font size=\"3\">\n",
        "The function $term\\_freq$ has the following input and output:\n",
        "<ul>\n",
        "<li> **Input.** A RDD $words$, where each element is a word in a text document $d$ (pre-processing already done).\n",
        "<li> **Output.** A RDD, where each element is a key-value pair $(w, tf(w, d))$.\n",
        "</ul>\n",
        "</font>\n",
        "</p>\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\" color='#91053d'>**Write the code of the function $term\\_freq$. You can take advantage of the \n",
        "    function $word\\_count$.**\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "<hr style=\" border:solid 2px;\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euZPF61WJu61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b95d279c-64ae-406e-86fd-9aa758fdbaea"
      },
      "source": [
        "def term_freq(words):\n",
        "    '''############## WRITE YOUR CODE HERE ##############'''\n",
        "    counts = word_count(words)\n",
        "    \n",
        "    sum_all = counts.map(lambda x: x[1]).reduce(lambda x, y: x+y)\n",
        "\n",
        "    raw_counts = counts.map(lambda x: (x[0], x[1] / sum_all)) \n",
        "    \n",
        "    return raw_counts\n",
        "\n",
        "    '''############## END OF THE EXERCISE ##############'''\n",
        " \n",
        "text = sc.textFile('./data/bbc/politics/001.txt')\n",
        "words = preprocess(text, stopwords)\n",
        "tf = term_freq(words)\n",
        "tf.take(5)\n",
        "\n",
        "################# EXPECTED OUTPUT #################\n",
        "################# THIS IS NOT THE SOLUTION #################\n",
        "#\n",
        "#[('a', 0,333),\n",
        "# ('b', 0.032),\n",
        "# ... ]\n",
        "#\n",
        "###################################################"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('pay', 0.045081967213114756),\n",
              " ('maternity', 0.036885245901639344),\n",
              " ('months', 0.036885245901639344),\n",
              " ('said', 0.036885245901639344),\n",
              " ('plans', 0.020491803278688523)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    }
  ]
}